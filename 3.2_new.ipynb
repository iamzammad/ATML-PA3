{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":158389,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":134606,"modelId":157354}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 3.2","metadata":{}},{"cell_type":"markdown","source":"# Data Loader","metadata":{}},{"cell_type":"code","source":"import torch\nimport torchvision\nimport torchvision.transforms as transforms\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import models\nimport math\nimport pandas as pd\n\ndef load_data(batch_size=128):\n    \"\"\"Load CIFAR-100 dataset.\"\"\"\n    transform_train = transforms.Compose([\n        transforms.RandomCrop(32, padding=4),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)),\n        transforms.Resize(224),\n    ])\n    \n    transform_test = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)),\n        transforms.Resize(224),\n    ])\n    \n    trainset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform_train)\n    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n    \n    testset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform_test)\n    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n    \n    return trainloader, testloader","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-11-06T20:05:09.614270Z","iopub.execute_input":"2024-11-06T20:05:09.614675Z","iopub.status.idle":"2024-11-06T20:05:15.400060Z","shell.execute_reply.started":"2024-11-06T20:05:09.614632Z","shell.execute_reply":"2024-11-06T20:05:15.398468Z"},"trusted":true},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"# Base Teacher Model - VGG16","metadata":{}},{"cell_type":"code","source":"class TeacherModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        model = models.vgg16(pretrained=True)\n        self.features = model.features\n        self.pool = nn.AdaptiveAvgPool2d((7, 7))\n        self.classifier = nn.Sequential(\n            nn.Linear(512 * 7 * 7, 4096),\n            nn.ReLU(True),\n            nn.Dropout(),\n            nn.Linear(4096, 4096),\n            nn.ReLU(True),\n            nn.Dropout(),\n            nn.Linear(4096, 100)\n        )\n        \n        for param in model.parameters():\n            param.requires_grad = False\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.pool(x)  \n        x = torch.flatten(x, 1)\n        x = self.classifier(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-11-06T20:05:20.305227Z","iopub.execute_input":"2024-11-06T20:05:20.305620Z","iopub.status.idle":"2024-11-06T20:05:20.313785Z","shell.execute_reply.started":"2024-11-06T20:05:20.305582Z","shell.execute_reply":"2024-11-06T20:05:20.312887Z"},"trusted":true},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# Basic Student Model - VGG11","metadata":{}},{"cell_type":"code","source":"from torchvision import models\n\nclass StudentModel(nn.Module):\n    def __init__(self):\n        super(StudentModel, self).__init__()\n        model = models.vgg11(pretrained=True) \n        self.features = model.features\n        model.classifier[-1] = nn.Linear(4096, 100)  \n        self.classifier = model.classifier\n\n    def forward(self, x):\n        x = self.features(x)\n        x = torch.flatten(x, 1)\n        x = self.classifier(x)\n        return x\n","metadata":{"execution":{"iopub.status.busy":"2024-11-06T20:05:25.552938Z","iopub.execute_input":"2024-11-06T20:05:25.553576Z","iopub.status.idle":"2024-11-06T20:05:25.560071Z","shell.execute_reply.started":"2024-11-06T20:05:25.553534Z","shell.execute_reply":"2024-11-06T20:05:25.559128Z"},"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"# Fine Tuning Basic Teacher Model","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom tqdm import tqdm\n\ndef train_teacher(model, trainloader, testloader, device, epochs=7):\n    \"\"\"Pretrain the teacher model on CIFAR-100.\"\"\"\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=5e-4)\n    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n    \n    best_acc = 0\n    for epoch in range(epochs):\n        model.train()\n        running_loss = 0.0\n        correct = 0\n        total = 0\n        \n        for inputs, labels in tqdm(trainloader, desc=f'Teacher Training Epoch {epoch + 1}/{epochs}'):\n            inputs, labels = inputs.to(device), labels.to(device)\n            optimizer.zero_grad()\n            \n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            \n            running_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += labels.size(0)\n            correct += predicted.eq(labels).sum().item()\n        \n        epoch_loss = running_loss / len(trainloader)\n        epoch_acc = 100. * correct / total\n        print(f'Teacher Training: Epoch {epoch + 1}, Loss: {epoch_loss:.3f}, Acc: {epoch_acc:.2f}%')\n        \n        model.eval()\n        correct = 0\n        total = 0\n        with torch.no_grad():\n            for inputs, labels in testloader:\n                inputs, labels = inputs.to(device), labels.to(device)\n                outputs = model(inputs)\n                _, predicted = outputs.max(1)\n                total += labels.size(0)\n                correct += predicted.eq(labels).sum().item()\n        \n        acc = 100. * correct / total\n        print(f'Teacher Epoch {epoch + 1}: Validation Accuracy: {acc:.2f}%')\n        \n        if acc > best_acc:\n            best_acc = acc\n            torch.save(model.state_dict(), 'best_teacher.pth')\n        \n        scheduler.step()\n    \n    model.load_state_dict(torch.load('best_teacher.pth'))\n    return model\n","metadata":{"execution":{"iopub.status.busy":"2024-11-04T11:52:48.838751Z","iopub.execute_input":"2024-11-04T11:52:48.839142Z","iopub.status.idle":"2024-11-04T11:52:48.852420Z","shell.execute_reply.started":"2024-11-04T11:52:48.839105Z","shell.execute_reply":"2024-11-04T11:52:48.851517Z"},"trusted":true},"outputs":[],"execution_count":4},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ntrainloader, testloader = load_data()\nteacher = TeacherModel().to(device)\nteacher = train_teacher(teacher, trainloader, testloader, device)\n\n# # Train with different KD methods\n# train_student_with_logit_matching(student, teacher, trainloader, device)\n# train_student_with_hints(student, teacher, trainloader, device)\n# train_student_with_crd(student, teacher, trainloader, device)\n","metadata":{"execution":{"iopub.status.busy":"2024-11-04T11:52:48.853761Z","iopub.execute_input":"2024-11-04T11:52:48.854092Z","iopub.status.idle":"2024-11-04T12:11:45.876502Z","shell.execute_reply.started":"2024-11-04T11:52:48.854058Z","shell.execute_reply":"2024-11-04T12:11:45.875339Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar-100-python.tar.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 169001437/169001437 [00:06<00:00, 27245895.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting ./data/cifar-100-python.tar.gz to ./data\nFiles already downloaded and verified\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n100%|██████████| 528M/528M [00:02<00:00, 208MB/s] \nTeacher Training Epoch 1/7: 100%|██████████| 391/391 [02:14<00:00,  2.90it/s]","output_type":"stream"},{"name":"stdout","text":"Teacher Training: Epoch 1, Loss: 2.441, Acc: 37.02%\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Teacher Epoch 1: Validation Accuracy: 57.74%\n","output_type":"stream"},{"name":"stderr","text":"Teacher Training Epoch 2/7: 100%|██████████| 391/391 [02:13<00:00,  2.92it/s]","output_type":"stream"},{"name":"stdout","text":"Teacher Training: Epoch 2, Loss: 1.551, Acc: 55.86%\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Teacher Epoch 2: Validation Accuracy: 62.64%\n","output_type":"stream"},{"name":"stderr","text":"Teacher Training Epoch 3/7: 100%|██████████| 391/391 [02:13<00:00,  2.93it/s]","output_type":"stream"},{"name":"stdout","text":"Teacher Training: Epoch 3, Loss: 1.342, Acc: 61.25%\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Teacher Epoch 3: Validation Accuracy: 64.97%\n","output_type":"stream"},{"name":"stderr","text":"Teacher Training Epoch 4/7: 100%|██████████| 391/391 [02:13<00:00,  2.93it/s]","output_type":"stream"},{"name":"stdout","text":"Teacher Training: Epoch 4, Loss: 1.200, Acc: 64.93%\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Teacher Epoch 4: Validation Accuracy: 66.58%\n","output_type":"stream"},{"name":"stderr","text":"Teacher Training Epoch 5/7: 100%|██████████| 391/391 [02:13<00:00,  2.92it/s]","output_type":"stream"},{"name":"stdout","text":"Teacher Training: Epoch 5, Loss: 1.092, Acc: 67.81%\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Teacher Epoch 5: Validation Accuracy: 67.79%\n","output_type":"stream"},{"name":"stderr","text":"Teacher Training Epoch 6/7: 100%|██████████| 391/391 [02:13<00:00,  2.92it/s]","output_type":"stream"},{"name":"stdout","text":"Teacher Training: Epoch 6, Loss: 1.000, Acc: 70.36%\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Teacher Epoch 6: Validation Accuracy: 68.60%\n","output_type":"stream"},{"name":"stderr","text":"Teacher Training Epoch 7/7: 100%|██████████| 391/391 [02:14<00:00,  2.92it/s]","output_type":"stream"},{"name":"stdout","text":"Teacher Training: Epoch 7, Loss: 0.956, Acc: 71.72%\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Teacher Epoch 7: Validation Accuracy: 69.04%\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_30/1939661751.py:64: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load('best_teacher.pth'))\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import zipfile\n\nzip_filename = 'best_teacher.zip'\nmodel_filename = 'best_teacher.pth'\nwith zipfile.ZipFile(zip_filename, 'w') as zipf:\n    zipf.write(model_filename)\n\nprint(f'Zipped {model_filename} into {zip_filename}')\n","metadata":{"execution":{"iopub.status.busy":"2024-11-04T12:11:45.878263Z","iopub.execute_input":"2024-11-04T12:11:45.879256Z","iopub.status.idle":"2024-11-04T12:11:47.208928Z","shell.execute_reply.started":"2024-11-04T12:11:45.879189Z","shell.execute_reply":"2024-11-04T12:11:47.207980Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Zipped best_teacher.pth into best_teacher.zip\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"# Finetuning Basic Student Model","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom tqdm import tqdm\nimport pandas as pd \n\ndef train_model(model, trainloader, valloader, criterion, optimizer, epochs=5, save_path=\"student_model.pth\"):\n    model.train()\n    \n    train_losses, train_accuracies = [], []\n    val_losses, val_accuracies = [], []\n    \n    for epoch in range(epochs):\n        running_loss = 0.0\n        correct = 0\n        total = 0\n        \n        for inputs, labels in tqdm(trainloader, desc=f'Epoch {epoch + 1}/{epochs}'):\n            inputs, labels = inputs.cuda(), labels.cuda()\n            optimizer.zero_grad()\n            \n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            \n            running_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += labels.size(0)\n            correct += predicted.eq(labels).sum().item()\n        \n        epoch_train_loss = running_loss / len(trainloader)\n        epoch_train_accuracy = 100 * correct / total\n        train_losses.append(epoch_train_loss)\n        train_accuracies.append(epoch_train_accuracy)\n        \n        val_loss, val_correct, val_total = 0.0, 0, 0\n        model.eval()\n        with torch.no_grad():\n            for inputs, labels in valloader:\n                inputs, labels = inputs.cuda(), labels.cuda()\n                outputs = model(inputs)\n                loss = criterion(outputs, labels)\n                val_loss += loss.item()\n                _, predicted = outputs.max(1)\n                val_total += labels.size(0)\n                val_correct += predicted.eq(labels).sum().item()\n        \n        epoch_val_loss = val_loss / len(valloader)\n        epoch_val_accuracy = 100 * val_correct / val_total\n        val_losses.append(epoch_val_loss)\n        val_accuracies.append(epoch_val_accuracy)\n        \n        print(f\"Epoch {epoch + 1}/{epochs} | Train Loss: {epoch_train_loss:.4f} | Train Acc: {epoch_train_accuracy:.2f}% | Val Loss: {epoch_val_loss:.4f} | Val Acc: {epoch_val_accuracy:.2f}%\")\n\n    torch.save(model.state_dict(), save_path)\n    print(f'Model saved as {save_path}')\n\n    metrics_df = pd.DataFrame({\n        'Epoch': range(1, epochs + 1),\n        'Train Loss': train_losses,\n        'Train Accuracy': train_accuracies,\n        'Val Loss': val_losses,\n        'Val Accuracy': val_accuracies\n    })\n\n    metrics_df.to_csv('basic_student_metrics.csv', index=False)\n    print('Training metrics saved to basic_student_metrics.csv')\n\n    return train_losses, train_accuracies, val_losses, val_accuracies\n","metadata":{"execution":{"iopub.status.busy":"2024-11-06T20:05:31.091063Z","iopub.execute_input":"2024-11-06T20:05:31.091425Z","iopub.status.idle":"2024-11-06T20:05:31.106248Z","shell.execute_reply.started":"2024-11-06T20:05:31.091380Z","shell.execute_reply":"2024-11-06T20:05:31.105451Z"},"trusted":true},"outputs":[],"execution_count":6},{"cell_type":"code","source":"\nif __name__ == '__main__':\n    trainloader, valloader = load_data()\n    \n    model = StudentModel().cuda()  \n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n\n    print(\"Fine-tuning for 2 epochs...\")\n    train_losses_2, train_accuracies_2, val_losses_2, val_accuracies_2 = train_model(\n        model, trainloader, valloader, criterion, optimizer, epochs=2, save_path=\"trained_student_model_2_epochs.pth\"\n    )\n    \n\n    \n# if __name__ == '__main__':\n#     main()","metadata":{"execution":{"iopub.status.busy":"2024-11-06T20:05:49.684070Z","iopub.execute_input":"2024-11-06T20:05:49.684733Z","iopub.status.idle":"2024-11-06T20:13:13.880728Z","shell.execute_reply.started":"2024-11-06T20:05:49.684692Z","shell.execute_reply":"2024-11-06T20:13:13.879550Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Files already downloaded and verified\nFiles already downloaded and verified\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG11_Weights.IMAGENET1K_V1`. You can also use `weights=VGG11_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/vgg11-8a719046.pth\" to /root/.cache/torch/hub/checkpoints/vgg11-8a719046.pth\n100%|██████████| 507M/507M [00:02<00:00, 221MB/s]  \n","output_type":"stream"},{"name":"stdout","text":"Fine-tuning for 2 epochs...\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/2: 100%|██████████| 391/391 [03:24<00:00,  1.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/2 | Train Loss: 2.0882 | Train Acc: 44.71% | Val Loss: 1.2398 | Val Acc: 63.44%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/2: 100%|██████████| 391/391 [03:23<00:00,  1.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/2 | Train Loss: 1.0513 | Train Acc: 68.85% | Val Loss: 1.0141 | Val Acc: 70.24%\nModel saved as trained_student_model_2_epochs.pth\nTraining metrics saved to basic_student_metrics.csv\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":" # Fine-tune the model for an additional 5 epochs\nprint(\"Fine-tuning for 5 additional epochs...\")\ntrain_losses_5, train_accuracies_5, val_losses_5, val_accuracies_5 = train_model(\n    model, trainloader, valloader, criterion, optimizer, epochs=5, save_path=\"trained_student_model_5_epochs.pth\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T20:45:17.420742Z","iopub.execute_input":"2024-11-06T20:45:17.421123Z","iopub.status.idle":"2024-11-06T20:56:29.590166Z","shell.execute_reply.started":"2024-11-06T20:45:17.421088Z","shell.execute_reply":"2024-11-06T20:56:29.588930Z"}},"outputs":[{"name":"stdout","text":"Fine-tuning for 3 additional epochs...\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/3: 100%|██████████| 391/391 [03:28<00:00,  1.87it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/3 | Train Loss: 0.9863 | Train Acc: 70.68% | Val Loss: 1.0177 | Val Acc: 70.27%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/3: 100%|██████████| 391/391 [03:28<00:00,  1.87it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/3 | Train Loss: 0.6447 | Train Acc: 80.21% | Val Loss: 0.9212 | Val Acc: 73.84%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/3: 100%|██████████| 391/391 [03:28<00:00,  1.87it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/3 | Train Loss: 0.5073 | Train Acc: 84.29% | Val Loss: 0.9602 | Val Acc: 73.59%\nModel saved as trained_student_model_5_epochs.pth\nTraining metrics saved to basic_student_metrics.csv\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"# Basic Logit Matching KD","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom tqdm import tqdm\nimport pandas as pd\n\ndef train_with_logit_matching(student, teacher, train_loader, device, \n                            temperature=2.0, alpha=0.5, epochs=5, save_path='training_metrics.csv'):\n    \"\"\"\n    Improved implementation of logit matching knowledge distillation.\n    \n    Args:\n        student: Student model\n        teacher: Teacher model\n        train_loader: DataLoader for training data\n        device: Device to run the models on\n        temperature: Temperature for softening logits (default: 2.0)\n        alpha: Weight for hard loss (default: 0.5)\n        epochs: Number of training epochs (default: 5)\n        save_path: Path to save the CSV file containing loss and accuracy\n    \"\"\"\n    optimizer = optim.Adam(student.parameters(), lr=1e-4, weight_decay=5e-4)\n    hard_criterion = nn.CrossEntropyLoss()\n    soft_criterion = nn.KLDivLoss(reduction=\"batchmean\")\n    \n    teacher.eval()  \n    epoch_losses = []\n    epoch_accuracies = []\n    \n    for epoch in range(epochs):\n        student.train()\n        running_loss = 0.0\n        correct = 0\n        total = 0\n        \n        for inputs, labels in train_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            optimizer.zero_grad()\n            \n            with torch.no_grad():\n                teacher_logits = teacher(inputs)\n            \n            student_logits = student(inputs)\n            soft_teacher = F.softmax(teacher_logits / temperature, dim=1)\n            log_soft_student = F.log_softmax(student_logits / temperature, dim=1)\n            \n            distillation_loss = soft_criterion(log_soft_student, soft_teacher) * (temperature ** 2)\n            hard_loss = hard_criterion(student_logits, labels)\n            loss = (1 - alpha) * distillation_loss + alpha * hard_loss\n            \n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n            \n            _, predicted = student_logits.max(1)\n            total += labels.size(0)\n            correct += predicted.eq(labels).sum().item()\n        \n        epoch_loss = running_loss / len(train_loader)\n        epoch_acc = 100. * correct / total\n        \n        epoch_losses.append(epoch_loss)\n        epoch_accuracies.append(epoch_acc)\n        \n        print(f'Epoch {epoch+1}: Loss = {epoch_loss:.3f}, Accuracy = {epoch_acc:.2f}%')\n    \n    metrics_df = pd.DataFrame({\n        'Epoch': list(range(1, epochs+1)),\n        'Loss': epoch_losses,\n        'Accuracy': epoch_accuracies\n    })\n    \n    metrics_df.to_csv(save_path, index=False)\n    \n    return student\n","metadata":{"execution":{"iopub.status.busy":"2024-11-06T20:17:39.423260Z","iopub.execute_input":"2024-11-06T20:17:39.423793Z","iopub.status.idle":"2024-11-06T20:17:39.440522Z","shell.execute_reply.started":"2024-11-06T20:17:39.423744Z","shell.execute_reply":"2024-11-06T20:17:39.439445Z"},"trusted":true},"outputs":[],"execution_count":9},{"cell_type":"code","source":"import torch\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\nteacher_model = TeacherModel().to(device)\nteacher_model.load_state_dict(torch.load(\"/kaggle/input/lmteacher/pytorch/default/1/best_teacher.pth\"))\nteacher_model.eval()  \n\nstudent_model = StudentModel().to(device)\nstudent_model.load_state_dict(torch.load(\"trained_student_model_2_epochs.pth\"))\nstudent_model.eval()  \n\ntrained_student_model = train_with_logit_matching(\n    student=student_model, \n    teacher=teacher_model, \n    train_loader=trainloader, \n    device=device, \n    epochs=5, \n    alpha=0.5, \n    temperature=2.0, \n    save_path=\"training_metrics.csv\"  \n)\n\ntorch.save(trained_student_model.state_dict(), \"student_with_logit_matching.pth\")\n\n","metadata":{"execution":{"iopub.status.busy":"2024-11-06T20:17:42.006851Z","iopub.execute_input":"2024-11-06T20:17:42.007217Z","iopub.status.idle":"2024-11-06T20:44:26.044997Z","shell.execute_reply.started":"2024-11-06T20:17:42.007183Z","shell.execute_reply":"2024-11-06T20:44:26.043872Z"},"trusted":true},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n100%|██████████| 528M/528M [00:02<00:00, 195MB/s] \n/tmp/ipykernel_30/3935964990.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  teacher_model.load_state_dict(torch.load(\"/kaggle/input/lmteacher/pytorch/default/1/best_teacher.pth\"))\n/tmp/ipykernel_30/3935964990.py:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  student_model.load_state_dict(torch.load(\"trained_student_model_2_epochs.pth\"))\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1: Loss = 1.024, Accuracy = 71.93%\nEpoch 2: Loss = 0.889, Accuracy = 74.58%\nEpoch 3: Loss = 0.826, Accuracy = 76.62%\nEpoch 4: Loss = 0.780, Accuracy = 78.18%\nEpoch 5: Loss = 0.745, Accuracy = 79.54%\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"def evaluate_model(model, testloader, device):\n    model.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for inputs, labels in testloader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            _, predicted = outputs.max(1)\n            total += labels.size(0)\n            correct += predicted.eq(labels).sum().item()\n    acc = 100. * correct / total\n    print(f'Model Test Accuracy: {acc:.2f}%')\n    return acc\n\n","metadata":{"execution":{"iopub.status.busy":"2024-11-04T12:56:27.081968Z","iopub.execute_input":"2024-11-04T12:56:27.082303Z","iopub.status.idle":"2024-11-04T12:56:27.089665Z","shell.execute_reply.started":"2024-11-04T12:56:27.082267Z","shell.execute_reply":"2024-11-04T12:56:27.088622Z"},"trusted":true},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"# Contrastive Representation Distillation","metadata":{}},{"cell_type":"markdown","source":"# CRD Student & Teacher\n","metadata":{}},{"cell_type":"code","source":"import torch\nimport torchvision\nimport torchvision.transforms as transforms\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import models\nimport pandas as pd\n\nclass TeacherModelCRD(nn.Module):\n    def __init__(self):\n        super().__init__()\n        model = models.vgg16(pretrained=True)\n        self.features = model.features\n        self.avgpool = model.avgpool\n        self.classifier = nn.Sequential(\n            nn.Linear(512 * 7 * 7, 4096),\n            nn.ReLU(True),\n            nn.Dropout(),\n            nn.Linear(4096, 4096),\n            nn.ReLU(True),\n            nn.Dropout(),\n            nn.Linear(4096, 100)\n        )\n        \n    def forward(self, x):\n        x = self.features(x)\n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n        x = self.classifier(x)\n        return x\n    \n    def get_features(self, x):\n        x = self.features(x)\n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n        x = self.classifier[:-1](x) \n        return x\n\nclass StudentModelCRD(nn.Module):\n    def __init__(self):\n        super().__init__()\n        \n        vgg11 = models.vgg11(pretrained=True)\n        self.features = vgg11.features \n        \n        self.classifier = nn.Sequential(\n            nn.Linear(512 * 7 * 7, 1024),\n            nn.ReLU(True),\n            nn.Dropout(),\n            nn.Linear(1024, 512),\n            nn.ReLU(True),\n            nn.Dropout(),\n            nn.Linear(512, 100) \n        )\n        \n        self.projection = nn.Sequential(\n            nn.Linear(512, 4096),\n            nn.ReLU(True)\n        )\n        \n        self.apply(self._init_weights)  \n        \n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            nn.init.normal_(m.weight, 0, 0.01)\n            nn.init.constant_(m.bias, 0)\n        \n    def forward(self, x):\n        x = self.features(x)\n        x = nn.AdaptiveAvgPool2d((7, 7))(x) \n        x = torch.flatten(x, 1)\n        x = self.classifier(x)\n        return x\n    \n    def get_features(self, x):\n        x = self.features(x)\n        x = nn.AdaptiveAvgPool2d((7, 7))(x)\n        x = torch.flatten(x, 1)\n        x = self.classifier[:-1](x)  \n        x = self.projection(x)  \n        return x","metadata":{"execution":{"iopub.status.busy":"2024-11-04T14:22:41.082199Z","iopub.execute_input":"2024-11-04T14:22:41.083037Z","iopub.status.idle":"2024-11-04T14:22:41.100567Z","shell.execute_reply.started":"2024-11-04T14:22:41.082993Z","shell.execute_reply":"2024-11-04T14:22:41.099492Z"},"trusted":true},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import MultiStepLR\nfrom tqdm import tqdm\n\ndef train_model(model, trainloader, testloader, device, epochs=1, lr=1e-4, model_name='model'):\n    \"\"\"Generic training loop for both teacher and student models with progress tracking.\"\"\"\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n    scheduler = MultiStepLR(optimizer, milestones=[60, 120, 160], gamma=0.2)\n    \n    best_acc = 0\n    for epoch in range(epochs):\n        model.train()\n        running_loss = 0.0\n        correct = 0\n        total = 0\n\n        with tqdm(total=len(trainloader), desc=f\"Epoch {epoch + 1}/{epochs}\", unit=\"batch\") as pbar:\n            for i, (inputs, labels) in enumerate(trainloader):\n                inputs, labels = inputs.to(device), labels.to(device)\n                optimizer.zero_grad()\n\n                outputs = model(inputs)\n                loss = criterion(outputs, labels)\n                loss.backward()\n                optimizer.step()\n\n                running_loss += loss.item()\n                _, predicted = outputs.max(1)\n                total += labels.size(0)\n                correct += predicted.eq(labels).sum().item()\n                \n                pbar.update(1)\n                pbar.set_postfix(loss=loss.item(), accuracy=100. * correct / total)\n\n        epoch_loss = running_loss / len(trainloader)\n        epoch_acc = 100. * correct / total\n        print(f'Training: Epoch {epoch + 1}, Loss: {epoch_loss:.3f}, Acc: {epoch_acc:.2f}%')\n        \n        model.eval()\n        correct = 0\n        total = 0\n        with torch.no_grad():\n            for inputs, labels in testloader:\n                inputs, labels = inputs.to(device), labels.to(device)\n                outputs = model(inputs)\n                _, predicted = outputs.max(1)\n                total += labels.size(0)\n                correct += predicted.eq(labels).sum().item()\n        \n        acc = 100. * correct / total\n        print(f'Epoch {epoch + 1}: Validation Accuracy: {acc:.2f}%')\n        \n        if acc > best_acc:\n            best_acc = acc\n            torch.save(model.state_dict(), f'{model_name}_best_model.pth')\n        \n        scheduler.step() \n    \n    model.load_state_dict(torch.load(f'{model_name}_best_model.pth'))\n    return model\n","metadata":{"execution":{"iopub.status.busy":"2024-11-04T14:22:44.765039Z","iopub.execute_input":"2024-11-04T14:22:44.765402Z","iopub.status.idle":"2024-11-04T14:22:44.780404Z","shell.execute_reply.started":"2024-11-04T14:22:44.765367Z","shell.execute_reply":"2024-11-04T14:22:44.779380Z"},"trusted":true},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"**Finetune teacher**","metadata":{}},{"cell_type":"code","source":"\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nbatch_size = 128\ntrainloader, testloader = load_data(batch_size)\n\nteacher_model = TeacherModelCRD().to(device)\nteacher_model = train_model(teacher_model, trainloader, testloader, device, epochs=7, lr=1e-4, model_name='teachercrd')\n","metadata":{"execution":{"iopub.status.busy":"2024-11-04T13:03:20.999743Z","iopub.execute_input":"2024-11-04T13:03:21.000090Z","iopub.status.idle":"2024-11-04T13:50:56.779930Z","shell.execute_reply.started":"2024-11-04T13:03:21.000058Z","shell.execute_reply":"2024-11-04T13:50:56.778820Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar-100-python.tar.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 169001437/169001437 [00:06<00:00, 24910360.99it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting ./data/cifar-100-python.tar.gz to ./data\nFiles already downloaded and verified\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n100%|██████████| 528M/528M [00:02<00:00, 202MB/s] \nEpoch 1/7: 100%|██████████| 391/391 [06:20<00:00,  1.03batch/s, accuracy=43.3, loss=1.28]","output_type":"stream"},{"name":"stdout","text":"Training: Epoch 1, Loss: 2.168, Acc: 43.26%\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1: Validation Accuracy: 65.54%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/7: 100%|██████████| 391/391 [06:19<00:00,  1.03batch/s, accuracy=64.2, loss=0.915]","output_type":"stream"},{"name":"stdout","text":"Training: Epoch 2, Loss: 1.252, Acc: 64.24%\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2: Validation Accuracy: 68.53%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/7: 100%|██████████| 391/391 [06:19<00:00,  1.03batch/s, accuracy=70.9, loss=1.02] ","output_type":"stream"},{"name":"stdout","text":"Training: Epoch 3, Loss: 0.998, Acc: 70.89%\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3: Validation Accuracy: 71.74%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/7: 100%|██████████| 391/391 [06:19<00:00,  1.03batch/s, accuracy=75.6, loss=0.655]","output_type":"stream"},{"name":"stdout","text":"Training: Epoch 4, Loss: 0.827, Acc: 75.58%\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4: Validation Accuracy: 73.58%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/7: 100%|██████████| 391/391 [06:19<00:00,  1.03batch/s, accuracy=78.7, loss=0.645]","output_type":"stream"},{"name":"stdout","text":"Training: Epoch 5, Loss: 0.709, Acc: 78.66%\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5: Validation Accuracy: 73.95%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/7: 100%|██████████| 391/391 [06:18<00:00,  1.03batch/s, accuracy=81.6, loss=0.75] ","output_type":"stream"},{"name":"stdout","text":"Training: Epoch 6, Loss: 0.606, Acc: 81.60%\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6: Validation Accuracy: 75.35%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/7: 100%|██████████| 391/391 [06:19<00:00,  1.03batch/s, accuracy=83.9, loss=0.54] ","output_type":"stream"},{"name":"stdout","text":"Training: Epoch 7, Loss: 0.524, Acc: 83.91%\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7: Validation Accuracy: 75.48%\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_30/417066355.py:68: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(f'{model_name}_best_model.pth'))\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"**Finetune Student**","metadata":{}},{"cell_type":"code","source":"\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nbatch_size = 128\ntrainloader, testloader = load_data(batch_size)\n\nstudent_model = StudentModelCRD().to(device)\nstudent_model = train_model(student_model, trainloader, testloader, device, epochs=2, lr=1e-4, model_name='studentcrd')\n","metadata":{"execution":{"iopub.status.busy":"2024-11-04T13:56:36.278088Z","iopub.execute_input":"2024-11-04T13:56:36.278479Z","iopub.status.idle":"2024-11-04T14:03:42.628942Z","shell.execute_reply.started":"2024-11-04T13:56:36.278443Z","shell.execute_reply":"2024-11-04T14:03:42.627718Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Files already downloaded and verified\nFiles already downloaded and verified\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG11_Weights.IMAGENET1K_V1`. You can also use `weights=VGG11_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/vgg11-8a719046.pth\" to /root/.cache/torch/hub/checkpoints/vgg11-8a719046.pth\n100%|██████████| 507M/507M [00:02<00:00, 209MB/s]  \nEpoch 1/2: 100%|██████████| 391/391 [03:14<00:00,  2.01batch/s, accuracy=23.6, loss=1.97]","output_type":"stream"},{"name":"stdout","text":"Training: Epoch 1, Loss: 3.096, Acc: 23.57%\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1: Validation Accuracy: 52.36%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/2: 100%|██████████| 391/391 [03:15<00:00,  2.00batch/s, accuracy=47, loss=1.97]  ","output_type":"stream"},{"name":"stdout","text":"Training: Epoch 2, Loss: 1.897, Acc: 46.95%\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2: Validation Accuracy: 60.41%\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_30/417066355.py:68: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(f'{model_name}_best_model.pth'))\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"# CRD LOSS","metadata":{}},{"cell_type":"code","source":"\nclass DistillKL(nn.Module):\n    def __init__(self, T):\n        super(DistillKL, self).__init__()\n        self.T = T\n\n    def forward(self, y_s, y_t):\n        p_s = nn.functional.log_softmax(y_s/self.T, dim=1)\n        p_t = nn.functional.softmax(y_t/self.T, dim=1)\n        loss = nn.KLDivLoss(reduction='batchmean')(p_s, p_t) * (self.T**2)\n        return loss\n\nclass ContrastiveLoss(nn.Module):\n    def __init__(self, temperature=0.07):\n        super(ContrastiveLoss, self).__init__()\n        self.temperature = temperature\n\n    def forward(self, student_features, teacher_features):\n        student_features = nn.functional.normalize(student_features, dim=1)\n        teacher_features = nn.functional.normalize(teacher_features, dim=1)\n\n        similarity = torch.matmul(student_features, teacher_features.T) / self.temperature\n        labels = torch.arange(similarity.size(0), device=similarity.device)\n        loss = nn.CrossEntropyLoss()(similarity, labels)\n        \n        return loss\n\n","metadata":{"execution":{"iopub.status.busy":"2024-11-04T14:22:54.150416Z","iopub.execute_input":"2024-11-04T14:22:54.151302Z","iopub.status.idle":"2024-11-04T14:22:54.160429Z","shell.execute_reply.started":"2024-11-04T14:22:54.151256Z","shell.execute_reply":"2024-11-04T14:22:54.159398Z"},"trusted":true},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# Train CRD Student & Teacher","metadata":{}},{"cell_type":"code","source":"import pandas as pd  \n\nfrom tqdm import tqdm  \n\ndef train_epoch(teacher_model, student_model, trainloader, device, optimizer, kl_criterion, contrastive_criterion):\n    student_model.train()\n    teacher_model.eval()\n    \n    total_loss = 0\n    total_samples = 0\n    kl_losses = []\n    contr_losses = []\n    cls_losses = []\n\n    progress_bar = tqdm(trainloader, desc=\"Training\", unit=\"batch\")\n\n    for batch_idx, (inputs, targets) in enumerate(progress_bar):\n        inputs, targets = inputs.to(device), targets.to(device)\n        batch_size = inputs.size(0)\n        \n        optimizer.zero_grad()\n        \n        with torch.no_grad():\n            teacher_outputs = teacher_model(inputs)\n            teacher_features = teacher_model.get_features(inputs)\n        \n        student_outputs = student_model(inputs)\n        student_features = student_model.get_features(inputs)\n        \n        kl_loss = kl_criterion(student_outputs, teacher_outputs)\n        contr_loss = contrastive_criterion(student_features, teacher_features)\n        cls_loss = nn.CrossEntropyLoss()(student_outputs, targets)\n        \n        loss = 0.5 * kl_loss + 0.1 * contr_loss + 0.4 * cls_loss\n        \n        loss.backward()\n        optimizer.step()\n        \n        total_loss += loss.item() * batch_size\n        total_samples += batch_size\n        \n        kl_losses.append(kl_loss.item())\n        contr_losses.append(contr_loss.item())\n        cls_losses.append(cls_loss.item())\n\n        progress_bar.set_postfix({\n            'Loss': loss.item(),\n            'KL Loss': kl_loss.item(),\n            'Contr Loss': contr_loss.item(),\n            'Cls Loss': cls_loss.item()\n        })\n\n    avg_loss = total_loss / total_samples\n    return avg_loss, kl_losses, contr_losses, cls_losses\n\n\ndef evaluate(model, dataloader, device):\n    model.eval()\n    correct = 0\n    total = 0\n    \n    with torch.no_grad():\n        for inputs, targets in dataloader:\n            inputs, targets = inputs.to(device), targets.to(device)\n            outputs = model(inputs)\n            _, predicted = outputs.max(1)\n            total += targets.size(0)\n            correct += predicted.eq(targets).sum().item()\n    \n    accuracy = 100. * correct / total\n    return accuracy\n\ndef train_and_save_metrics(teacher_model, student_model, trainloader, eval_loader, device, optimizer, kl_criterion, contrastive_criterion, epochs=5):\n    all_kl_losses = []\n    all_contr_losses = []\n    all_cls_losses = []\n    \n    for epoch in range(epochs):\n        print(f'Epoch {epoch + 1}/{epochs}')\n        avg_loss, kl_losses, contr_losses, cls_losses = train_epoch(\n            teacher_model, student_model, trainloader, device, optimizer, kl_criterion, contrastive_criterion\n        )\n        \n        all_kl_losses.extend(kl_losses)\n        all_contr_losses.extend(contr_losses)\n        all_cls_losses.extend(cls_losses)\n\n        eval_accuracy = evaluate(student_model, eval_loader, device)\n        print(f'Evaluation Accuracy: {eval_accuracy:.2f}%')\n\n    metrics_df = pd.DataFrame({\n        'KL Loss': all_kl_losses,\n        'Contrastive Loss': all_contr_losses,\n        'Classification Loss': all_cls_losses\n    })\n\n    metrics_df.to_csv('crd_metrics.csv', index=False)\n    print('Training metrics saved to crd_metrics.csv')\n","metadata":{"execution":{"iopub.status.busy":"2024-11-04T15:26:10.684406Z","iopub.execute_input":"2024-11-04T15:26:10.684823Z","iopub.status.idle":"2024-11-04T15:26:10.704366Z","shell.execute_reply.started":"2024-11-04T15:26:10.684783Z","shell.execute_reply":"2024-11-04T15:26:10.703349Z"},"trusted":true},"outputs":[],"execution_count":9},{"cell_type":"code","source":"\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\nbatch_size = 128\nnum_epochs = 5\nlearning_rate = 1e-4\ntemperature = 4.0\n\ntrainloader, testloader = load_data(batch_size)\n\nteacher_model = TeacherModelCRD().to(device)\nteacher_model.load_state_dict(torch.load('/kaggle/input/crdteacher/pytorch/default/1/teachercrd_best_model.pth'))\nteacher_model.eval()  \n\n\nstudent_model_crd = StudentModelCRD().to(device)\ntry:\n    student_model_crd.load_state_dict(torch.load('/kaggle/input/crdstudent/pytorch/default/1/studentcrd_best_model.pth'))\nexcept FileNotFoundError:\n    print(\"No pre-trained student model found, starting from scratch.\")\n\noptimizer_crd = optim.Adam(student_model_crd.parameters(), lr=learning_rate)\nkl_criterion = DistillKL(temperature) \ncontrastive_criterion = ContrastiveLoss()  \n\nprint(\"\\nStarting training with Contrastive Representation Distillation...\")\ntrain_and_save_metrics(\n    teacher_model, student_model_crd, trainloader, testloader, \n    device, optimizer_crd, kl_criterion, contrastive_criterion, \n    epochs=num_epochs\n)\n\ntorch.save(student_model_crd.state_dict(), 'CRDfinal_student_model.pth')\n","metadata":{"execution":{"iopub.status.busy":"2024-11-04T15:26:18.732473Z","iopub.execute_input":"2024-11-04T15:26:18.733170Z","iopub.status.idle":"2024-11-04T16:21:37.441842Z","shell.execute_reply.started":"2024-11-04T15:26:18.733127Z","shell.execute_reply":"2024-11-04T16:21:37.440817Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Using device: cuda\nFiles already downloaded and verified\nFiles already downloaded and verified\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_30/3269298347.py:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  teacher_model.load_state_dict(torch.load('/kaggle/input/crdteacher/pytorch/default/1/teachercrd_best_model.pth'))\n/tmp/ipykernel_30/3269298347.py:21: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  student_model_crd.load_state_dict(torch.load('/kaggle/input/crdstudent/pytorch/default/1/studentcrd_best_model.pth'))\n","output_type":"stream"},{"name":"stdout","text":"\nStarting training with Contrastive Representation Distillation...\nEpoch 1/5\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 391/391 [10:47<00:00,  1.66s/batch, Loss=3, KL Loss=4.68, Contr Loss=1.02, Cls Loss=1.4]    \n","output_type":"stream"},{"name":"stdout","text":"Evaluation Accuracy: 64.16%\nEpoch 2/5\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 391/391 [10:47<00:00,  1.66s/batch, Loss=3.03, KL Loss=4.71, Contr Loss=0.779, Cls Loss=1.49] \n","output_type":"stream"},{"name":"stdout","text":"Evaluation Accuracy: 67.21%\nEpoch 3/5\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 391/391 [10:47<00:00,  1.66s/batch, Loss=2.08, KL Loss=3.02, Contr Loss=0.667, Cls Loss=1.27] \n","output_type":"stream"},{"name":"stdout","text":"Evaluation Accuracy: 68.43%\nEpoch 4/5\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 391/391 [10:47<00:00,  1.66s/batch, Loss=2.55, KL Loss=3.68, Contr Loss=0.667, Cls Loss=1.62] \n","output_type":"stream"},{"name":"stdout","text":"Evaluation Accuracy: 69.55%\nEpoch 5/5\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 391/391 [10:47<00:00,  1.66s/batch, Loss=2.14, KL Loss=3.25, Contr Loss=0.673, Cls Loss=1.12] \n","output_type":"stream"},{"name":"stdout","text":"Evaluation Accuracy: 70.19%\nTraining metrics saved to crd_metrics.csv\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"# Hint-Based Distillation","metadata":{}},{"cell_type":"markdown","source":"# Updated Student & Teacher models with Hint layers\n","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import models\nfrom torch.utils.data import DataLoader\n\nclass HintTeacherModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        model = models.vgg16(pretrained=True)\n        self.features = model.features\n        self.classifier = nn.Sequential(\n            nn.Linear(25088, 4096), \n            nn.ReLU(True),\n            nn.Dropout(),\n            nn.Linear(4096, 4096),\n            nn.ReLU(True),\n            nn.Dropout(),\n            nn.Linear(4096, 100)\n        )\n        \n        for param in model.parameters():\n            param.requires_grad = False\n        \n    def forward(self, x):\n        features = self.features(x)\n        hint_features = features \n        x = torch.flatten(features, 1)\n        logits = self.classifier(x)\n        \n        return logits, hint_features\n\n\nclass HintStudentModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        model = models.vgg11(pretrained=True)\n        self.features = model.features\n        \n        dummy_input = torch.randn(1, 3, 224, 224)  \n        dummy_output = self.features(dummy_input)\n        flattened_size = dummy_output.view(dummy_output.size(0), -1).size(1) \n        self.regressor = nn.Conv2d(512, 512, kernel_size=1)\n\n        self.classifier = nn.Sequential(\n            nn.Linear(flattened_size, 2048),  \n            nn.BatchNorm1d(2048),\n            nn.ReLU(True),\n            nn.Dropout(0.3),\n            nn.Linear(2048, 1024),\n            nn.BatchNorm1d(1024),\n            nn.ReLU(True),\n            nn.Dropout(0.3),\n            nn.Linear(1024, 100)\n        )\n        \n        self.apply(self._init_weights)\n        \n    def _init_weights(self, m):\n        if isinstance(m, nn.Conv2d):\n            nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n            if m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.BatchNorm2d):\n            nn.init.constant_(m.weight, 1)\n            nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.Linear):\n            nn.init.normal_(m.weight, 0, 0.01)\n            nn.init.constant_(m.bias, 0)\n            \n    def forward(self, x):\n        features = self.features(x)\n        hint_features = self.regressor(features) \n        x = torch.flatten(features, 1)\n        logits = self.classifier(x)\n        \n        return logits, hint_features\n","metadata":{"execution":{"iopub.status.busy":"2024-11-04T18:15:47.502487Z","iopub.execute_input":"2024-11-04T18:15:47.503002Z","iopub.status.idle":"2024-11-04T18:15:47.521048Z","shell.execute_reply.started":"2024-11-04T18:15:47.502964Z","shell.execute_reply":"2024-11-04T18:15:47.519734Z"},"trusted":true},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# Training student & teacher","metadata":{}},{"cell_type":"code","source":"def train_model(model, train_loader, val_loader, device, epochs=5, model_type='teacher'):\n    \"\"\"Train the model (teacher or student) with specified hyperparameters.\"\"\"\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=1e-4) \n    \n    best_acc = 0.0\n    \n    for epoch in range(epochs):\n        model.train()\n        running_loss = 0.0\n        correct = 0\n        total = 0\n        \n        for inputs, labels in train_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            \n            optimizer.zero_grad()\n            outputs, _ = model(inputs) \n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            \n            running_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += labels.size(0)\n            correct += predicted.eq(labels).sum().item()\n        \n        train_acc = 100. * correct / total\n        model.eval()\n        correct = 0\n        total = 0\n        \n        with torch.no_grad():\n            for inputs, labels in val_loader:\n                inputs, labels = inputs.to(device), labels.to(device)\n                outputs, _ = model(inputs) \n                _, predicted = outputs.max(1)\n                total += labels.size(0)\n                correct += predicted.eq(labels).sum().item()\n        \n        val_acc = 100. * correct / total\n        print(f'Epoch {epoch + 1}: Train Loss = {running_loss / len(train_loader):.4f}, '\n              f'Train Acc = {train_acc:.2f}%, Val Acc = {val_acc:.2f}%')\n        \n        if val_acc > best_acc:\n            best_acc = val_acc\n            torch.save(model.state_dict(), f'best_{model_type}_hint.pth')\n    \n    model.load_state_dict(torch.load(f'best_{model_type}_hint.pth'))\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-11-04T18:15:53.603287Z","iopub.execute_input":"2024-11-04T18:15:53.603749Z","iopub.status.idle":"2024-11-04T18:15:53.618871Z","shell.execute_reply.started":"2024-11-04T18:15:53.603704Z","shell.execute_reply":"2024-11-04T18:15:53.617761Z"},"trusted":true},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"**Finetuning teacher and student**","metadata":{}},{"cell_type":"code","source":"\ntrain_loader, val_loader = load_data(128)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nteacher_model = HintTeacherModel().to(device)\ntrain_model(teacher_model, train_loader, val_loader, device, epochs=7, model_type='teacher')\n","metadata":{"execution":{"iopub.status.busy":"2024-11-04T16:33:02.048017Z","iopub.execute_input":"2024-11-04T16:33:02.048880Z","iopub.status.idle":"2024-11-04T16:51:29.796758Z","shell.execute_reply.started":"2024-11-04T16:33:02.048839Z","shell.execute_reply":"2024-11-04T16:51:29.795606Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Files already downloaded and verified\nFiles already downloaded and verified\nEpoch 1: Train Loss = 2.4476, Train Acc = 37.16%, Val Acc = 57.69%\nEpoch 2: Train Loss = 1.5551, Train Acc = 55.61%, Val Acc = 62.72%\nEpoch 3: Train Loss = 1.3483, Train Acc = 60.79%, Val Acc = 65.71%\nEpoch 4: Train Loss = 1.2241, Train Acc = 64.23%, Val Acc = 67.27%\nEpoch 5: Train Loss = 1.1343, Train Acc = 66.71%, Val Acc = 67.79%\nEpoch 6: Train Loss = 1.0503, Train Acc = 68.71%, Val Acc = 67.48%\nEpoch 7: Train Loss = 0.9869, Train Acc = 70.60%, Val Acc = 68.83%\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_30/713316720.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(f'best_{model_type}_hint.pth'))\n","output_type":"stream"},{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"HintTeacherModel(\n  (features): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU(inplace=True)\n    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU(inplace=True)\n    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (6): ReLU(inplace=True)\n    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (8): ReLU(inplace=True)\n    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (11): ReLU(inplace=True)\n    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (13): ReLU(inplace=True)\n    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (15): ReLU(inplace=True)\n    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (18): ReLU(inplace=True)\n    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (20): ReLU(inplace=True)\n    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (22): ReLU(inplace=True)\n    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (25): ReLU(inplace=True)\n    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (27): ReLU(inplace=True)\n    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (29): ReLU(inplace=True)\n    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (classifier): Sequential(\n    (0): Linear(in_features=25088, out_features=4096, bias=True)\n    (1): ReLU(inplace=True)\n    (2): Dropout(p=0.5, inplace=False)\n    (3): Linear(in_features=4096, out_features=4096, bias=True)\n    (4): ReLU(inplace=True)\n    (5): Dropout(p=0.5, inplace=False)\n    (6): Linear(in_features=4096, out_features=100, bias=True)\n  )\n)"},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ntrain_loader, val_loader = load_data(128)\nstudent_model = HintStudentModel().to(device)\ntrain_model(student_model, train_loader, val_loader, device, epochs=2, model_type='student')","metadata":{"execution":{"iopub.status.busy":"2024-11-04T17:28:27.635956Z","iopub.execute_input":"2024-11-04T17:28:27.636388Z","iopub.status.idle":"2024-11-04T17:35:32.967542Z","shell.execute_reply.started":"2024-11-04T17:28:27.636346Z","shell.execute_reply":"2024-11-04T17:35:32.966490Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Files already downloaded and verified\nFiles already downloaded and verified\nEpoch 1: Train Loss = 3.8314, Train Acc = 12.69%, Val Acc = 21.26%\nEpoch 2: Train Loss = 3.1699, Train Acc = 23.48%, Val Acc = 30.34%\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_30/713316720.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(f'best_{model_type}_hint.pth'))\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"HintStudentModel(\n  (features): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU(inplace=True)\n    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (4): ReLU(inplace=True)\n    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (7): ReLU(inplace=True)\n    (8): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (9): ReLU(inplace=True)\n    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (11): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (12): ReLU(inplace=True)\n    (13): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (14): ReLU(inplace=True)\n    (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (16): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (17): ReLU(inplace=True)\n    (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (19): ReLU(inplace=True)\n    (20): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (regressor): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n  (classifier): Sequential(\n    (0): Linear(in_features=25088, out_features=2048, bias=True)\n    (1): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n    (3): Dropout(p=0.3, inplace=False)\n    (4): Linear(in_features=2048, out_features=1024, bias=True)\n    (5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (6): ReLU(inplace=True)\n    (7): Dropout(p=0.3, inplace=False)\n    (8): Linear(in_features=1024, out_features=100, bias=True)\n  )\n)"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"import pandas as pd \nfrom tqdm import tqdm \n\ndef train_hint_distillation(teacher, student, train_loader, val_loader, epochs, device, \n                           hint_weight=0.25, ce_weight=0.75):\n    \"\"\"Train the student model using hint-based distillation\"\"\"\n    teacher.eval()\n    student.train()\n    \n    criterion = nn.CrossEntropyLoss()\n    hint_criterion = nn.MSELoss()\n    optimizer = optim.SGD(student.parameters(),\n                         lr=0.01,\n                         momentum=0.9,\n                         weight_decay=5e-4)\n    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n    best_acc = 0.0\n    \n    all_train_losses = []\n    all_val_losses = []\n    all_train_accuracies = []\n    all_val_accuracies = []\n    \n    for epoch in range(epochs):\n        running_loss = 0.0\n        correct = 0\n        total = 0\n        \n        for inputs, labels in tqdm(train_loader, desc=f'Training Epoch {epoch + 1}/{epochs}', leave=False):\n            inputs, labels = inputs.to(device), labels.to(device)\n            \n            with torch.no_grad():\n                _, teacher_hints = teacher(inputs)\n            \n            optimizer.zero_grad()\n            student_logits, student_hints = student(inputs)\n            \n            ce_loss = criterion(student_logits, labels)\n            hint_loss = hint_criterion(student_hints, teacher_hints)\n            total_loss = ce_weight * ce_loss + hint_weight * hint_loss\n            \n            total_loss.backward()\n            optimizer.step()\n            \n            running_loss += total_loss.item()\n            _, predicted = student_logits.max(1)\n            total += labels.size(0)\n            correct += predicted.eq(labels).sum().item()\n            \n        train_acc = 100. * correct / total\n        \n        all_train_losses.append(running_loss / len(train_loader))\n        all_train_accuracies.append(train_acc)\n        \n        student.eval()\n        correct = 0\n        total = 0\n        val_loss = 0.0\n        \n        with torch.no_grad():\n            for inputs, labels in tqdm(val_loader, desc=f'Validation Epoch {epoch + 1}/{epochs}', leave=False):\n                inputs, labels = inputs.to(device), labels.to(device)\n                outputs, _ = student(inputs)\n                _, predicted = outputs.max(1)\n                total += labels.size(0)\n                correct += predicted.eq(labels).sum().item()\n                val_loss += criterion(outputs, labels).item()\n        \n        student.train()\n        val_acc = 100. * correct / total\n        \n        all_val_losses.append(val_loss / len(val_loader))\n        all_val_accuracies.append(val_acc)\n        \n        print(f'Epoch {epoch + 1}: Loss = {running_loss / len(train_loader):.4f}, '\n              f'Train Acc = {train_acc:.2f}%, Val Acc = {val_acc:.2f}%')\n        \n        if val_acc > best_acc:\n            best_acc = val_acc\n            torch.save(student.state_dict(), 'best_student_hint_distillation.pth')\n            \n        scheduler.step()\n        \n    metrics_df = pd.DataFrame({\n        'Epoch': range(1, epochs + 1),\n        'Train Loss': all_train_losses,\n        'Train Accuracy': all_train_accuracies,\n        'Val Loss': all_val_losses,\n        'Val Accuracy': all_val_accuracies\n    })\n\n\n    metrics_df.to_csv('hint_metrics.csv', index=False)\n    print('Training metrics saved to hint_metrics.csv')\n","metadata":{"execution":{"iopub.status.busy":"2024-11-04T18:15:59.546028Z","iopub.execute_input":"2024-11-04T18:15:59.546419Z","iopub.status.idle":"2024-11-04T18:15:59.563581Z","shell.execute_reply.started":"2024-11-04T18:15:59.546383Z","shell.execute_reply":"2024-11-04T18:15:59.562503Z"},"trusted":true},"outputs":[],"execution_count":4},{"cell_type":"code","source":"import torch\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ntrainloader, testloader = load_data(128)\n\nteacher = HintTeacherModel().to(device)\nstudent = HintStudentModel().to(device)\n\nteacher.load_state_dict(torch.load('/kaggle/input/hintteacher/pytorch/default/1/best_teacher_hint.pth'))\nprint(\"Loaded teacher model.\")\n\nstudent.load_state_dict(torch.load('/kaggle/input/hintstudent/pytorch/default/1/best_student_hint.pth'))\nprint(\"Loaded student model.\")\n\nprint(\"Training student with hint-based distillation...\")\ntrain_hint_distillation(\n    teacher=teacher,\n    student=student,\n    train_loader=trainloader,\n    val_loader=testloader,\n    epochs=5,\n    device=device,\n    hint_weight=0.25,\n    ce_weight=0.75\n)\n\ntorch.save(student.state_dict(), 'best_student_hint_distillation.pth')\nprint(\"Finished training and saved the student model.\")\n\ndef evaluate_model(model, testloader, device):\n    model.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for inputs, labels in testloader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs, _ = model(inputs)\n            _, predicted = outputs.max(1)\n            total += labels.size(0)\n            correct += predicted.eq(labels).sum().item()\n    return 100. * correct / total\n\nteacher_acc = evaluate_model(teacher, testloader, device)\nstudent_acc = evaluate_model(student, testloader, device)\n\nprint(f\"\\nFinal Results:\")\nprint(f\"Teacher Accuracy: {teacher_acc:.2f}%\")\nprint(f\"Student Accuracy: {student_acc:.2f}%\")\n","metadata":{"execution":{"iopub.status.busy":"2024-11-04T18:30:38.384928Z","iopub.execute_input":"2024-11-04T18:30:38.385406Z","iopub.status.idle":"2024-11-04T18:58:03.103923Z","shell.execute_reply.started":"2024-11-04T18:30:38.385361Z","shell.execute_reply":"2024-11-04T18:58:03.102663Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Files already downloaded and verified\nFiles already downloaded and verified\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_30/2015401095.py:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  teacher.load_state_dict(torch.load('/kaggle/input/hintteacher/pytorch/default/1/best_teacher_hint.pth'))\n","output_type":"stream"},{"name":"stdout","text":"Loaded teacher model.\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_30/2015401095.py:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  student.load_state_dict(torch.load('/kaggle/input/hintstudent/pytorch/default/1/best_student_hint.pth'))\n","output_type":"stream"},{"name":"stdout","text":"Loaded student model.\nTraining student with hint-based distillation...\n","output_type":"stream"},{"name":"stderr","text":"                                                                     \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1: Loss = 2.1697, Train Acc = 29.25%, Val Acc = 30.41%\n","output_type":"stream"},{"name":"stderr","text":"                                                                     \r","output_type":"stream"},{"name":"stdout","text":"Epoch 2: Loss = 1.9207, Train Acc = 35.75%, Val Acc = 38.79%\n","output_type":"stream"},{"name":"stderr","text":"                                                                     \r","output_type":"stream"},{"name":"stdout","text":"Epoch 3: Loss = 1.7229, Train Acc = 41.46%, Val Acc = 44.88%\n","output_type":"stream"},{"name":"stderr","text":"                                                                     \r","output_type":"stream"},{"name":"stdout","text":"Epoch 4: Loss = 1.5703, Train Acc = 46.02%, Val Acc = 49.95%\n","output_type":"stream"},{"name":"stderr","text":"                                                                     \r","output_type":"stream"},{"name":"stdout","text":"Epoch 5: Loss = 1.4560, Train Acc = 49.68%, Val Acc = 52.79%\nTraining metrics saved to hint_metrics.csv\nFinished training and saved the student model.\n\nFinal Results:\nTeacher Accuracy: 68.83%\nStudent Accuracy: 52.79%\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Data\nmethods = ['Basic Student', 'Logit Matching', 'CRD', 'Hint', 'Teacher']\naccuracies = [74.70, 81.69, 70.19, 56.79, 75.72]\n\nplt.figure(figsize=(8, 6))\nplt.bar(methods, accuracies, color=['#aec6cf', '#f8c8a0', '#b5e2b3', '#f8b7b3', '#f7e59b'])\n\nplt.xlabel('Methods')\nplt.ylabel('Validation Accuracy (%)')\nplt.title('Validation Accuracy for Different Models')\n\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T21:18:37.334252Z","iopub.execute_input":"2024-11-06T21:18:37.334691Z","iopub.status.idle":"2024-11-06T21:18:37.645926Z","shell.execute_reply.started":"2024-11-06T21:18:37.334633Z","shell.execute_reply":"2024-11-06T21:18:37.644784Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 800x600 with 1 Axes>","image/png":"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"},"metadata":{}}],"execution_count":1}]}